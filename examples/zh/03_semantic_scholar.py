"""è¯­ä¹‰å­¦è€… - æ¼”ç¤ºå‘é‡æœç´¢å’ŒæŒä¹…åŒ–åŠŸèƒ½ã€‚

æ­¤ç¤ºä¾‹æ„å»ºäº†ä¸€ä¸ªå…·æœ‰è¯­ä¹‰æœç´¢åŠŸèƒ½çš„ç ”ç©¶è®ºæ–‡åº“ã€‚
ç”¨æˆ·å¯ä»¥æ ¹æ®å†…å®¹ç›¸ä¼¼æ€§è€Œä¸ä»…ä»…æ˜¯å…³é”®å­—æ¥æœç´¢è®ºæ–‡ï¼Œ
å¹¶ä¸”è¯¥åº“ä¸ºæœªæ¥çš„ä¼šè¯ä¿ç•™å…¶çŠ¶æ€ã€‚

ä¸»è¦ç‰¹æ€§ï¼š
- è¯­ä¹‰ç›¸ä¼¼æ€§çš„å‘é‡æœç´¢ï¼ˆéœ€è¦OpenAIåµŒå…¥ï¼‰
- ä¸å‘é‡æœç´¢å¹¶è¡Œçš„é”®å€¼æŸ¥æ‰¾
- è®ºæ–‡å’ŒåµŒå…¥çš„æŒä¹…åŒ–å­˜å‚¨
- æ‰¹é‡ç´¢å¼•å’Œæœç´¢æ“ä½œ
"""

import json
from pathlib import Path
from pydantic import BaseModel
from dotenv import load_dotenv

from ontomem import OMem

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœå¯ç”¨åˆ™åŠ è½½OPENAI_API_KEYï¼‰
load_dotenv()


class ResearchPaper(BaseModel):
    """ç ”ç©¶è®ºæ–‡çš„å…ƒæ•°æ®å’Œæ‘˜è¦ã€‚"""

    paper_id: str
    title: str
    authors: list[str]
    abstract: str
    year: int
    citations: int = 0
    keywords: list[str] = []
    related_papers: list[str] = []


def example_semantic_scholar():
    """æ¼”ç¤ºè®ºæ–‡åº“ä¸­çš„è¯­ä¹‰æœç´¢å’ŒæŒä¹…åŒ–ã€‚"""
    print("\n" + "=" * 80)
    print("è¯­ä¹‰å­¦è€…ï¼šå…·æœ‰å‘é‡æœç´¢åŠŸèƒ½çš„ç ”ç©¶è®ºæ–‡åº“")
    print("=" * 80)

    # ç¤ºä¾‹ç ”ç©¶è®ºæ–‡ - è‡ªç„¶è¯­è¨€å¤„ç†ç„¦ç‚¹
    nlp_papers = [
        ResearchPaper(
            paper_id="nlp_001",
            title="æ³¨æ„åŠ›å°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡",
            authors=["Vaswani, A.", "Shazeer, N.", "Parmar, N."],
            abstract="ä¸»æµçš„åºåˆ—è½¬å¯¼æ¨¡å‹åŸºäºå¤æ‚çš„å¾ªç¯æˆ–å·ç§¯ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç®€å•ç½‘ç»œæ¶æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘ˆå¼ƒäº†é€’æ¨å’Œå·ç§¯ã€‚",
            year=2017,
            citations=88000,
            keywords=["transformer", "attention", "è‡ªç„¶è¯­è¨€å¤„ç†"],
            related_papers=["nlp_002", "nlp_003"],
        ),
        ResearchPaper(
            paper_id="nlp_002",
            title="BERTï¼šæ·±åº¦åŒå‘Transformerçš„é¢„è®­ç»ƒ",
            authors=["Devlin, J.", "Chang, M.", "Lee, K."],
            abstract="æˆ‘ä»¬ä»‹ç»BERTï¼Œä¸€ç§é¢„è®­ç»ƒè¯­è¨€è¡¨ç¤ºçš„æ–¹æ³•ï¼Œåœ¨å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè·å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚",
            year=2018,
            citations=65000,
            keywords=["BERT", "è¯­è¨€æ¨¡å‹", "é¢„è®­ç»ƒ"],
            related_papers=["nlp_001", "nlp_004"],
        ),
        ResearchPaper(
            paper_id="nlp_003",
            title="è¯­è¨€æ¨¡å‹æ˜¯æ— ç›‘ç£çš„å¤šä»»åŠ¡å­¦ä¹ å™¨",
            authors=["Radford, A.", "Wu, J.", "Child, R."],
            abstract="GPT-2æ¼”ç¤ºäº†è¯­è¨€æ¨¡å‹åœ¨æ–°æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶ï¼Œåœ¨æ²¡æœ‰ä»»ä½•æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹å¼€å§‹å­¦ä¹ è¿™äº›ä»»åŠ¡ã€‚",
            year=2019,
            citations=26000,
            keywords=["GPT-2", "è¯­è¨€ç”Ÿæˆ", "æ— ç›‘ç£"],
            related_papers=["nlp_001", "nlp_005"],
        ),
        ResearchPaper(
            paper_id="nlp_004",
            title="RoBERTaï¼šç»è¿‡ç¨³å¥ä¼˜åŒ–çš„BERTé¢„è®­ç»ƒæ–¹æ³•",
            authors=["Liu, Y.", "Ott, M.", "Goyal, N."],
            abstract="æˆ‘ä»¬ä»‹ç»RoBERTaï¼Œä¸€ç§ä¼˜åŒ–çš„è‡ªç›‘ç£è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…å«å…³é”®çš„è®­ç»ƒæµç¨‹ä¿®æ”¹ã€‚",
            year=2019,
            citations=15000,
            keywords=["RoBERTa", "BERT", "ä¼˜åŒ–"],
            related_papers=["nlp_002", "nlp_001"],
        ),
        ResearchPaper(
            paper_id="nlp_005",
            title="GPT-3ï¼šè¯­è¨€æ¨¡å‹æ˜¯å°‘æ ·æœ¬å­¦ä¹ å™¨",
            authors=["Brown, T.", "Mann, B.", "Ryder, N."],
            abstract="æœ€è¿‘çš„å·¥ä½œé€šè¿‡åœ¨å¤šæ ·åŒ–çš„è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒå¹¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œåœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å’ŒåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚",
            year=2020,
            citations=35000,
            keywords=["GPT-3", "å°‘æ ·æœ¬å­¦ä¹ ", "è¯­è¨€æ¨¡å‹"],
            related_papers=["nlp_003", "nlp_001"],
        ),
    ]

    # ç¤ºä¾‹ç ”ç©¶è®ºæ–‡ - è®¡ç®—æœºè§†è§‰ç„¦ç‚¹
    cv_papers = [
        ResearchPaper(
            paper_id="cv_001",
            title="ä¸€å¼ å›¾åƒå€¼1000ä¸ª16x16çš„å­—ï¼šç”¨äºå›¾åƒè¯†åˆ«çš„Transformer",
            authors=["Dosovitskiy, A.", "Beyer, L.", "Kolesnikov, A."],
            abstract="è™½ç„¶Transformeræ¶æ„å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†çš„äº‹å®æ ‡å‡†ï¼Œä½†å…¶åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œç›´æ¥åº”ç”¨äºå›¾åƒå—åºåˆ—çš„çº¯Transformeråœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å¾ˆå¥½ã€‚",
            year=2020,
            citations=22000,
            keywords=["vision transformer", "å›¾åƒåˆ†ç±»", "transformer"],
            related_papers=["cv_002", "cv_003"],
        ),
        ResearchPaper(
            paper_id="cv_002",
            title="Swin Transformerï¼šä½¿ç”¨ç§»åŠ¨çª—å£çš„åˆ†å±‚è§†è§‰Transformer",
            authors=["Liu, Z.", "Lin, Y.", "Cao, Y."],
            abstract="ä¸€ä¸ªç§°ä¸ºSwin Transformerçš„æ–°å‹è§†è§‰Transformerï¼Œå¯ä»¥å……åˆ†ç”¨ä½œè®¡ç®—æœºè§†è§‰çš„é€šç”¨éª¨å¹²ã€‚Swin Transformeré€šè¿‡ç”¨åŸºäºç§»åŠ¨çª—å£çš„æ³¨æ„åŠ›æ¨¡å—æ›¿æ¢æ¯ä¸ªTransformerå—ä¸­çš„æ ‡å‡†å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—æ¥æ„å»ºã€‚",
            year=2021,
            citations=15000,
            keywords=["Swin", "åˆ†å±‚", "è§†è§‰"],
            related_papers=["cv_001", "cv_004"],
        ),
        ResearchPaper(
            paper_id="cv_003",
            title="æ©ç›–è‡ªç¼–ç å™¨æ˜¯å¯æ‰©å±•çš„è§†è§‰å­¦ä¹ å™¨",
            authors=["He, K.", "Chen, X.", "Xie, S."],
            abstract="æœ¬æ–‡è¡¨æ˜æ©ç›–è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰æ˜¯è®¡ç®—æœºè§†è§‰çš„å¯æ‰©å±•è‡ªç›‘ç£å­¦ä¹ å™¨ã€‚æˆ‘ä»¬çš„MAEæ–¹æ³•å¾ˆç®€å•ï¼šæˆ‘ä»¬æ©ç›–è¾“å…¥å›¾åƒçš„éšæœºå—å¹¶é‡å»ºä¸¢å¤±çš„åƒç´ ã€‚",
            year=2021,
            citations=8000,
            keywords=["è‡ªç›‘ç£", "è§†è§‰", "æ©ç›–"],
            related_papers=["cv_001", "cv_005"],
        ),
        ResearchPaper(
            paper_id="cv_004",
            title="DeiTï¼šæ•°æ®é«˜æ•ˆçš„å›¾åƒTransformer",
            authors=["Touvron, H.", "Cord, M.", "Douze, M."],
            abstract="å›¾åƒåˆ†ç±»æ–¹é¢çš„æœ€è¿‘è¿›å±•ï¼Œæ— è®ºæ˜¯åœ¨å­¦æœ¯ç•Œè¿˜æ˜¯å·¥ä¸šç•Œï¼Œéƒ½æ˜¯ç”±ä»è‡ªç„¶è¯­è¨€å¤„ç†æ”¹è¿›çš„Transformeræ¨¡å‹é©±åŠ¨çš„ã€‚ç„¶è€Œï¼Œè¿™äº›è§†è§‰Transformerçš„è®¡ç®—å¯†é›†åº¦å¾ˆé«˜ï¼Œä¸å·ç§¯ç¥ç»ç½‘ç»œç›¸æ¯”ï¼Œåœ¨è¾¾åˆ°é«˜ç²¾åº¦æ—¶éœ€è¦æ˜æ˜¾æ›´å¤šçš„æ•°æ®ã€‚",
            year=2020,
            citations=10000,
            keywords=["DeiT", "é«˜æ•ˆ", "æ•°æ®é«˜æ•ˆ"],
            related_papers=["cv_001", "cv_002"],
        ),
        ResearchPaper(
            paper_id="cv_005",
            title="DINOï¼šè‡ªç›‘ç£è§†è§‰Transformerä¸­çš„æ–°å…´ç‰¹æ€§",
            authors=["Caron, M.", "Touvron, H.", "Misra, I."],
            abstract="æˆ‘ä»¬ç ”ç©¶è§†è§‰Transformerçš„æ–°å…´è‡ªç›‘ç£ç‰¹æ€§ï¼Œå¹¶è§‚å¯Ÿå®ƒä»¬åŒ…å«å…³äºå›¾åƒåŒºåŸŸå’Œç±»çš„æ˜ç¡®è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºDINOï¼Œä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ å’Œæ— æ ‡ç­¾çŸ¥è¯†è’¸é¦çš„æ–¹æ³•ã€‚",
            year=2021,
            citations=6000,
            keywords=["DINO", "è‡ªç›‘ç£", "è’¸é¦"],
            related_papers=["cv_001", "cv_003"],
        ),
    ]

    all_papers = nlp_papers + cv_papers

    print("\nğŸ“š åŠ è½½ç ”ç©¶è®ºæ–‡ï¼š")
    print("\n   ğŸ“ è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶æ–¹å‘ï¼š")
    for paper in nlp_papers:
        print(f"      [{paper.paper_id}] {paper.title} ({paper.year})")

    print("\n   ğŸ–¼ï¸  è®¡ç®—æœºè§†è§‰æ–¹å‘ï¼š")
    for paper in cv_papers:
        print(f"      [{paper.paper_id}] {paper.title} ({paper.year})")

    # åˆå§‹åŒ–OMem
    print("\nğŸ”§ æ­£åœ¨åˆå§‹åŒ–è®ºæ–‡åº“...")
    from ontomem.merger import MergeStrategy
    
    try:
        from langchain_openai import OpenAIEmbeddings
        from langchain_community.vectorstores import FAISS

        embedder = OpenAIEmbeddings(model="text-embedding-3-small")
        print("   âœ… æ‰¾åˆ°äº†OpenAI APIå¯†é’¥ - å·²å¯ç”¨å‘é‡æœç´¢")

        library = OMem(
            memory_schema=ResearchPaper,
            key_extractor=lambda x: x.paper_id,
            llm_client=None,
            embedder=embedder,
            strategy_or_merger=MergeStrategy.MERGE_FIELD,
        )
    except Exception as e:
        print(f"   âš ï¸  OpenAIä¸å¯ç”¨ - ä»…ä½¿ç”¨å…³é”®å­—æœç´¢")
        library = OMem(
            memory_schema=ResearchPaper,
            key_extractor=lambda x: x.paper_id,
            llm_client=None,
            embedder=None,
            strategy_or_merger=MergeStrategy.MERGE_FIELD,
        )

    # å°†è®ºæ–‡æ·»åŠ åˆ°åº“ä¸­
    print("\nğŸ“– æ­£åœ¨å‘ç»Ÿä¸€ç ”ç©¶åº“æ·»åŠ è®ºæ–‡...")
    library.add(all_papers)
    print(f"   åº“ä¸­çš„è®ºæ–‡æ€»æ•°ï¼š{library.size}")
    print(f"      â€¢ è‡ªç„¶è¯­è¨€å¤„ç†è®ºæ–‡ï¼š{len(nlp_papers)}")
    print(f"      â€¢ è®¡ç®—æœºè§†è§‰è®ºæ–‡ï¼š{len(cv_papers)}")

    # ä»ä¸¤ä¸ªæ–¹å‘æŒ‰IDæ£€ç´¢
    print("\nğŸ” ç›´æ¥æŸ¥æ‰¾ï¼ˆæ¥è‡ªä¸¤ä¸ªæ–¹å‘çš„è®ºæ–‡ï¼‰ï¼š")
    sample_papers = ["nlp_001", "cv_001"]
    for paper_id in sample_papers:
        paper = library.get(paper_id)
        if paper:
            track = "è‡ªç„¶è¯­è¨€å¤„ç†" if paper_id.startswith("nlp") else "è®¡ç®—æœºè§†è§‰"
            print(f"\n   [{track}] {paper.title}")
            print(f"      æ‘˜è¦ï¼š{paper.abstract[:80]}...")
            print(f"      å¼•ç”¨æ¬¡æ•°ï¼š{paper.citations:,}")

    # æ¼”ç¤ºè¯­ä¹‰æœç´¢ï¼ˆå¦‚æœåµŒå…¥å¯ç”¨ï¼‰
    print("\nğŸ¯ è¯­ä¹‰æœç´¢ç¤ºä¾‹ï¼š")
    print("-" * 80)

    search_queries = [
        "Transformerç¥ç»ç½‘ç»œ",
        "è§†è§‰å›¾åƒè¯†åˆ«",
        "è‡ªç›‘ç£å­¦ä¹ ",
    ]

    for query in search_queries:
        print(f"\n   æŸ¥è¯¢ï¼š'{query}'")
        try:
            # å°è¯•è¯­ä¹‰æœç´¢
            results = library.search(query, k=2)
            if results:
                print("   ç»“æœï¼ˆæŒ‰è¯­ä¹‰ç›¸ä¼¼æ€§æ’åºï¼‰ï¼š")
                for i, paper_result in enumerate(results, 1):
                    print(f"      {i}. {paper_result.title}")
                    print(f"         æ–¹å‘ï¼š{'è‡ªç„¶è¯­è¨€å¤„ç†' if paper_result.paper_id.startswith('nlp') else 'è®¡ç®—æœºè§†è§‰'}")
            else:
                print("   ï¼ˆè¯­ä¹‰æœç´¢éœ€è¦OpenAI APIå¯†é’¥ï¼‰")
        except Exception:
            # åå¤‡ï¼šå…³é”®å­—æœç´¢
            matching = [
                p
                for p in all_papers
                if any(kw.lower() in query.lower() for kw in p.keywords)
            ]
            if matching:
                print("   ç»“æœï¼ˆæŒ‰å…³é”®å­—åŒ¹é…ï¼‰ï¼š")
                for i, p in enumerate(matching[:2], 1):
                    print(f"      {i}. {p.title}")

    # å°†åº“ä¿å­˜åˆ°ç£ç›˜
    temp_dir = Path(__file__).parent.parent / "temp"
    temp_dir.mkdir(exist_ok=True)
    library_folder = temp_dir / "scholar_library"

    print(f"\nğŸ’¾ æ­£åœ¨å°†åº“ä¿å­˜åˆ°{library_folder.relative_to(temp_dir.parent)}...")
    library.dump(str(library_folder))
    print("   âœ… åº“å·²ä¿å­˜")

    # ç»Ÿè®¡æ•°æ®
    print("\nğŸ“Š åº“çš„ç»Ÿè®¡æ•°æ®ï¼š")
    print("-" * 80)
    total_citations = sum(p.citations for p in all_papers)
    avg_year = sum(p.year for p in all_papers) / len(all_papers)
    all_keywords = set()
    for p in all_papers:
        all_keywords.update(p.keywords)

    print(f"   è®ºæ–‡æ€»æ•°ï¼š{len(all_papers)}")
    print(f"      â€¢ è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ï¼š{len(nlp_papers)}ç¯‡è®ºæ–‡ï¼ˆ{sum(p.citations for p in nlp_papers):,}æ¬¡å¼•ç”¨ï¼‰")
    print(f"      â€¢ è®¡ç®—æœºè§†è§‰ï¼š{len(cv_papers)}ç¯‡è®ºæ–‡ï¼ˆ{sum(p.citations for p in cv_papers):,}æ¬¡å¼•ç”¨ï¼‰")
    print(f"\n   æ€»å¼•ç”¨æ•°ï¼š{total_citations:,}")
    print(f"   å¹³å‡å‘è¡¨å¹´ä»½ï¼š{avg_year:.0f}")
    print(f"   ç‹¬ç‰¹å…³é”®å­—ï¼š{len(all_keywords)}")
    print(f"   æœ€è¢«å¼•ç”¨çš„è®ºæ–‡ï¼š{max(all_papers, key=lambda p: p.citations).title}")

    # æ˜¾ç¤ºçƒ­é—¨å…³é”®å­—
    keyword_freq = {}
    for p in all_papers:
        for kw in p.keywords:
            keyword_freq[kw] = keyword_freq.get(kw, 0) + 1

    print(f"\n   çƒ­é—¨å…³é”®å­—ï¼ˆè·¨æ‰€æœ‰æ–¹å‘ï¼‰ï¼š")
    for kw, freq in sorted(keyword_freq.items(), key=lambda x: -x[1])[:5]:
        print(f"      â€¢ {kw}ï¼š{freq}ç¯‡è®ºæ–‡")

    print("\n" + "=" * 80)
    print("âœ¨ ç ”ç©¶è®ºæ–‡åº“å·²å‡†å¤‡å¥½è¿›è¡Œæ¢ç´¢ï¼")
    print("=" * 80)


if __name__ == "__main__":
    example_semantic_scholar()
